{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e77f9d-d9ef-483f-9f9f-969160ceb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.docstore.document import Document\n",
    "import warnings\n",
    "import getpass\n",
    "import PyPDF2\n",
    "import ollama\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29437be-4a20-4b32-8a73-6f3fce973c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\" # setting protobuf env vars up\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e39afa-79d0-4187-9d05-28d12052d039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'research_paper_final.pdf'}, page_content=\"From Signs to Speech: An End-to-End Conversational Platform for Deaf and Mute Individuals Using GRU and LLM Integration\\n\\nAryan Chauhan Department of Electrical and Electronic Engineering MIT World Peace University Pune, India chauhanaryan381@gmail.com\\n\\nAbdulqadir Kayamkhani Department of Electrical and Electronic Engineering MIT World Peace University Pune, India abdulqadirk2153@gmail.com\\n\\nAtharva Gujar Department of Electrical and Electronic Engineering MIT World Peace University Pune, India atharva.gujar7488@gmail.com\\n\\nManisha Kumawat Department of Electrical and Electronic Engineering MIT World Peace University Pune, India manisha.kumawat@mitwpu.edu.in\\n\\nMandar Gade\\n\\nDepartment of Electrical and Electronic Engineering MIT World Peace University Pune, India mandargade724@gmail.com\\n\\nindividuals are often disadvantaged in professional interview settings due to limited verbal relevant qualifications. This paper presents an AI-driven system designed to facilitate seamless bidirectional communication language recognition and speech through real-time sign synthesis. A custom dataset of ten technical signs—formulated through surveys in special education institutions—was recorded using MediaPipe and OpenCV. A three-layer Gated Recurrent Unit (GRU) model achieved 97% training accuracy and 94% test accuracy, outperforming LSTM and BiGRU architectures. Dropout regularization was applied to mitigate overfitting. A locally hosted Ollama LLM model was employed to enhance grammatical accuracy of sign-to-text-to-voice outputs. The interface, developed using Streamlit, supports user interaction, while Firebase manages backend communication. Precision and recall values of 93% and 92% respectively demonstrate the system’s reliability. This work proposes a deployable, inclusive solution aimed at improving accessibility and opportunity for deaf-mute individuals in professional environments.\\n\\nAbstract— Deaf and mute\\n\\ncommunication,\\n\\ndespite\\n\\npossessing\\n\\nKeywords—sign language recognition, deep learning, LLM,\\n\\ncomputer vision\\n\\nI. INTRODUCTION\\n\\nProfessional interviews present a unique challenge for deaf and mute individuals, particularly due to the lack of standardized sign terms. This communication gap limits the ability of differently-abled individuals to express professional competencies, restricting opportunities in employment. To address this, the paper proposes a real-time sign language recognition system that translates sign gestures into both text and speech, enabling effective interaction between interviewers and interviewees.\\n\\nlanguage for\\n\\ntechnical\\n\\nThe system integrates MediaPipe for landmark detection, OpenCV for video processing, TensorFlow for gesture classification, and a GRU-based model trained on ten custom technical signs relevant to professional settings. A locally hosted Ollama LLM further refines the translated sentences for grammatical clarity. Communication is facilitated through a Streamlit interface, with Firebase handling backend data flow, and Vosk enabling speech-to-text for interviewers.\\n\\ndomain-specific technical signs; (3) train and evaluate a GRU model for gesture classification; (4) improve linguistic output using LLM integration; and (5) develop an intuitive interface ensuring inclusive interaction.\\n\\nThe paper is organized as follows: Section II covers related work; Section III explains the system architecture; Section IV details implementation and training; Section V presents experimental results; Section VI concludes with key findings; and Section VII suggests future improvements for broader application.\\n\\nII. LITERATURE SURVEY\\n\\nIn [1], an attention-enhanced BiGRU network was introduced for dynamic gesture recognition using skeletal hand data extracted from RGB video. The incorporation of temporal attention into the BiGRU layers improved the model’s ability to focus on key transitional frames, achieving a 94.2% accuracy on a 20-class dynamic gesture dataset. A study in [2] proposed a Capsule Network architecture for static sign recognition, leveraging vector orientation features over scalar-based CNN outputs. Evaluated on the RWTH-BOSTON dataset, the model demonstrated superior rotational invariance and achieved a 96.8% accuracy, outperforming traditional CNNs in hand pose generalization. To improve latency, [3] introduced a Spatio-Temporal Separable Convolutional Network (STSCN) that separates temporal and spatial learning. Tested on real-time ASL gesture streams, the model reduced computational cost by 30% and maintained over 93% accuracy on a 25-class dataset. The authors in [4] proposed a cross-modal learning approach where hand landmarks and optical flow were jointly modelled using dual-stream GRU encoders. Fusion through attention yielded a classification accuracy of 92.6% on the WLASL dataset, highlighting the benefit of motion encoding. In [5], a temporal graph convolutional network (T-GCN) was applied to video sequences where each frame was represented as a graph of hand and face landmarks. The model achieved 94.5% accuracy on the MS-ASL dataset and edge-weight by offered contributions to classification. The work in [6] utilized depth-sensing and point cloud\\n\\ninterpretability\\n\\nvisualizing\\n\\nThe objectives of this study are to: (1) design a real-time sign- to-speech system for interviews; (2) create a dataset of\\n\\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE\\n\\ndata from Intel RealSense to train a 3D CNN-LSTM hybrid. Depth cues significantly improved gesture separation and yielded 95.7% accuracy on a 15-class ISL dataset. A lightweight temporal dilated GRU model was proposed in [7], which spaced out recurrent connections to capture long-term dependencies efficiently. On a constrained hardware device (Jetson Nano), it achieved 92% accuracy at 20 fps, offering practical deployment for mobile applications. In [8], the researchers explored zero-shot learning for SLR using attribute-based embeddings. By encoding gestures with semantic vectors, the model generalized to unseen signs during inference, achieving 78.4% accuracy on an extended ASL dataset. The study in [9] proposed a 1D CNN-GRU hybrid that reduced time-series dimensionality convolutions before feeding to recurrent layers. This reduced model complexity and achieved 91% accuracy on the SIGNUM dataset, with a 40% reduction in training time. Lastly, [10] examined the role of multimodal emotion recognition in sign language context to improve sentence- level accuracy. By integrating facial expression cues via facial landmark models with sign gestures, the system improved contextual disambiguation, reaching 90.5% accuracy on sentence-level ISL sequences.\\n\\nfeature\\n\\nusing\\n\\nIII. PROPOSED ARCHITECTURE\\n\\nThe system architecture is designed to enable smooth, real- time communication between deaf-mute individuals and interviewers through a modular, interactive web platform. The proposed framework supports both roles with role- specific processing pipelines that integrate computer vision, deep learning, natural language processing, and cloud-based communication. Users begin by accessing a centralized Streamlit-based web interface, where they register and log in. Upon entry, users are prompted to choose between the roles of interviewer or interviewee. If the user selects “interviewer”, they are directed to a module where speech is recorded and transcribed using the Vosk speech-to-text engine. The converted text is then transmitted in real time to the interviewee interface using Firebase’s real-time database services. On the interviewee side, the user activates the webcam stream, which is processed using OpenCV and MediaPipe for landmark detection. The system focuses specifically on the hands and face regions to extract key points for gesture recognition. These landmark sequences are passed into a trained Gated Recurrent Unit (GRU) model, optimized through experimentation for temporal classification of custom technical signs. The model output is translated into a preliminary sentence, which is then refined by a locally hosted Ollama LLM. The language model improves the sentence structure, adds missing functional words, and outputs a coherent sentence. This final message is delivered back to the interviewer interface, where it is converted into audio using the pyttsx3 text-to-speech engine. Firebase ensures synchronized communication between the two users throughout the interaction. To avoid misclassification from idle frames, a motion\\n\\nFig 1. : System Architecture\\n\\ndetection block ensures that the model only predicts when significant hand movement is detected. This mitigates noise and improves prediction accuracy during live usage. This architecture, illustrated in Fig. 1, creates a robust ecosystem for inclusive and fluent interview communication through synchronized deep learning, cloud services, and user- friendly design.\\n\\nIV. IMPLEMENTATION\\n\\nA. Custom Vocabulary and dataset creation To overcome the limitations of existing Indian Sign Language (ISL) datasets, which lack representations for many technical and workplace-specific terms, a custom set of 10 technical signs was created. These include database, graduation, science, software, backup, network, internet, spreadsheet, shortcut, and keyboard. These terms are frequently used in interviews and academic discussions but are not represented in standard ISL corpora, making them essential for enabling meaningful conversations in professional settings.\\n\\nB. Data Pre-processing To prepare the raw video data for model training, a structured preprocessing pipeline was implemented using Mediapipe, OpenCV, and NumPy. The Mediapipe Holistic model was employed to extract body landmarks, specifically capturing 21 points from the left hand, 21 points from the right hand, and 468 facial landmarks, along with static landmarks of the body resulting in a total of 1662 features per frame. For visualization and debugging, to render the mp_drawing_styles modules were used extracted key points. Gesture data was recorded using OpenCV, with 30 videos per sign captured at 30 frames per second (FPS). Each frame’s landmarks were converted into\\n\\nand\\n\\nmp_drawing\\n\\nNumPy arrays and stored as .npy files, resulting in 30 .npy files per video, which together encoded the full gesture sequence.\\n\\nFig. 2: Face and Hand Landmarks\\n\\nThe files were organized in directories based on the sign class name. A label map was created to assign numeric labels (0–9) to each of the ten custom technical signs. These .npy sequences were then grouped into sliding windows of 30 frames to form time-series input suitable for sequential model training. This preprocessing pipeline effectively transformed raw visual input into consistent, high-dimensional temporal data optimized for gesture classification. Fig. 2 neatly describes the landmarks captured by the mediapipe\\n\\nC. Model Architecture\\n\\nThe proposed gesture classification model adopts a three stacked Gated sequential structure comprising Recurrent Unit (GRU) layers followed by a series of fully connected dense layers. The input to the model is a 3D tensor of shape (30,1662), (30, 1662), (30,1662), representing 30 consecutive frames, each containing 1662 features extracted using the MediaPipe Holistic model. These features include landmark coordinates from the entire upper body; however, for the purpose of this application, only landmarks from the hands (21 points each for left and right) and the face (468 points) were relevant to the task. The remaining landmarks were either static or non-contributory and were included for uniformity in data representation but did not influence classification performance. The model begins with two GRU and layers, return_sequences=True, to capture temporal dependencies across the entire input sequence. A third GRU layer with 64 units and return_sequences=False condenses the learned temporal features into a fixed-dimensional context vector. This is followed by three dense layers with 64, 32, and 10 neurons, respectively. The final layer uses the softmax activation function to classify input sequences into one of the ten predefined custom sign classes. With a total of 832, 298 parameters, including 831,658 trainable and only 640 non- trainable parameters, the model is efficiently structured to maximize learning capacity while maintaining minimal\\n\\neach\\n\\nconfigured with\\n\\n128\\n\\nunits\\n\\ncomputational overhead. Table I: depicts the entire summary of the model.\\n\\nTable I: Model Summary\\n\\nNumber of GRU Layers GRU units per layer Activation function\\n\\nDense Layer sizes Trainable Parameters Non-Trainable Parameters Total Parameters\\n\\n3 128, 128, 64 Tanh (GRU), Relu (Dense), Softmax (Output) 64, 32, 10 831,658 640 832,298\\n\\nD. Model Training\\n\\nThe dataset was split into 80% training and 20% testing data, ensuring balanced representation across all classes. To ensure optimal convergence and generalization, training incorporated early stopping, learning rate reduction on plateau, and model checkpointing based on validation accuracy. Table II depicts the entire summary of the model training hyperparameters.\\n\\nTable II: Training Hyperparameters\\n\\nParameter Optimizer Learning Rate Loss Function Evaluation Metric Epochs Batch Size Early Stopping Patience ReduceLROnPlateau Patience Learning Rate Reduction Factor Model Checkpoint Metric Validation Accuracy\\n\\nValue Adam 0.0003 Categorical Crossentropy Categorical Accuracy 100 16 15 7\\n\\n0.2\\n\\nE. Regularization During experimentation, the model showed a tendency to overfit, particularly with deeper configurations and small dataset sizes. To address this, regularization was applied in the form of dropout and batch normalization. These techniques effectively reduced validation loss fluctuations and narrowed the gap between training and test accuracy. Table III depicts the regularization the summary of parameters of the trained model.\\n\\nTable III: Regularization Parameters\\n\\nParameter Dropout units per GRU layer Dropout units per Dense layer Batch Normalization\\n\\nValue 0.4, 0.4, 0.3\\n\\n0.3, 0.2, 0.2\\n\\nApplied after every GRU layer. (No BatchNorm for dense)\\n\\nF. Frontend Integration and Backend Communication\\n\\nThe frontend interface was developed using Streamlit, an open-source Python framework designed for deploying interactive machine learning applications. The interface allows users to register, log in, and select roles as either an interviewer or an interviewee. The role-based dashboard dynamically adapts the UI and functionalities, enabling distinct workflows for both user types.\\n\\nFig. 3: Frontend of the website\\n\\nTo enable seamless real-time communication between interviewee across different devices, interviewer and Firebase Realtime Database was integrated as the backend. It serves as a cloud-based synchronization hub, handling authentication, storage of transcribed or predicted messages, and facilitating low-latency, bidirectional data flow between both interfaces. Messages sent from the interviewer (voice- to-text) or the interviewee (sign-to-text) are instantly pushed and retrieved through Firebase’s real-time listener events. Fig. 3 is a brief visual representation of the frontend of the application.\\n\\nG. Voice Input via Vosk\\n\\nTo capture and process spoken queries from the interviewer, the system incorporates the Vosk speech recognition engine, powered by the GigaSpeech 0.42 GB lightweight ASR model. This configuration enables efficient voice-to-text transcription entirely offline, maintaining user privacy and reducing latency. The interviewer selects a voice input duration, after which Vosk captures the audio, processes it locally, and returns the corresponding transcribed text. This text is then routed via Firebase to the interviewee interface for interpretation and sign-based response.\\n\\nH. Class-to-Text conversion Pipeline\\n\\nThe GRU model outputs a class label representing one of the ten predefined technical words. These predicted labels are passed through a conversion dictionary that maps numeric class indices to textual words (e.g “2” is “science”). Detected words are appended sequentially to construct a rough sentence representation of the signed message. This raw sentence is then passed into the language processing module for correction and refinement.\\n\\nI. Sentence Correction via Ollama LLM\\n\\nsentence completeness, the assembled raw sentence is processed using a locally hosted 4 GB Ollama Large Language Model (LLM). The LLM is fine-tuned to correct sentence structure and fill in missing functional words such as “is,” “the,” or “my,” which sign-based communication. This step transforms a series of isolated\\n\\nTo enhance grammatical\\n\\nfluency\\n\\nand\\n\\nare often omitted\\n\\nin\\n\\nabsolute\\n\\ntechnical terms into a coherent, grammatically correct sentence that can be easily understood by the interviewer.\\n\\nJ. Text-to-Speech Conversion\\n\\nOnce the corrected sentence is generated by the LLM, it is sent back to the interviewer interface via Firebase. There, text-to-speech library—an offline Python the pyttsx3 engine—converts the final text into audible speech. This allows the interviewer to receive a natural voice response generated from the sign inputs of the deaf-mute interviewee, thereby completing the communication cycle fluently without delays and breaks.\\n\\nK. Motion-Based Prediction Trigger\\n\\nTo prevent false predictions during idle periods or unintentional hand movements, a motion detection block was integrated into the system. The model only initiates sign classification when sufficient hand motion is detected across consecutive frames. This logic filters out noise and stabilizes real-time predictions, ensuring that words are only generated when intentional signs are performed. This significantly improved accuracy and reduced unnecessary system responses during non-signing intervals.\\n\\nL. Model Evaluation\\n\\nThe final GRU model achieved a training accuracy of 96.67% and a test accuracy of 94%, demonstrating strong generalization and robustness across unseen data. In addition to accuracy, evaluation metrics such as precision, recall, F1- score, to comprehensively assess the model's performance. These metrics are summarized in Fig. 3 for clarity and comparison.\\n\\nand\\n\\nconfidence were\\n\\nalso\\n\\ncomputed\\n\\nM. Model Real Time Testing\\n\\nReal-time testing was performed using live webcam input across various devices and lighting conditions to assess model responsiveness and consistency. The GRU model accurately predicted sign classes within 1.5 to 2 seconds from gesture input to voice output, confirming its suitability for conversational use. Testing with multiple users showed consistent the motion-triggering and mechanism effectively minimized false predictions during idle frames. Fig. 4 shows all the metrics of the real-time tested words.\\n\\nperformance,\\n\\nFig. 4: Real time testing for words\\n\\nOverall, the system demonstrated robust and reliable\\n\\nbehavior in real-time settings, supporting its deployment in professional interviews involving deaf and mute individuals.\\n\\nFig. 5: Real time testing\\n\\nFig. 5 depicts the real time visual of the GRU prediction through cv2 window.\\n\\nFig. 6: Confusion Matrix\\n\\nFig. 6: Determines the confusion matrix of the final model.\\n\\nV. EXPERIMENTATION\\n\\nA. Approach and Architecture Tuning\\n\\nThe experimentation phase of this study involved design, training, and comparison of multiple deep sequential models to identify the most effective architecture for recognizing a custom vocabulary of ten technical sign language terms. These signs were developed through field interaction with special education institutions, where it was identified that deaf and mute individuals lacked standardized gestures for expressing common technical concepts, especially during interviews. Each model—GRU, LSTM, BiGRU, and BiLSTM—was trained using a dataset comprising 30 videos per sign, recorded at 30 frames per second which captured the temporal movements of hands and face landmarks over the camera X-Y plane. This was converted into a three- dimensional vector of sequence 30 which was fed as input\\n\\nthat was used across all models to ensure experimental consistency. trials, LSTM and BiLSTM showed During early acceptable performance for up to 7 classes; however, their training became unstable when scaled to all 10 classes. The learning curves fluctuated significantly, and test accuracy to overfitting—especially when deeper dropped due architectures were employed. For example, configurations such as 64-128-64-128 or 128-64-64-128 exhibited sharp increases in validation loss (exceeding 0.5) and showed poor convergence. Similar patterns were observed with BiGRU models, which also demonstrated sensitivity to layer depth. To address these issues, dropout layers were applied after each recurrent and dense layer, and batch normalization was introduced training. Multiple architectural combinations were tested, including GRU, LSTM, and BiGRU stacks with different layer sizes (e.g., 128-128-64 for temporal model layers and 64-32-10 for dense layers). After iterative fine-tuning, the GRU-based model delivered the most stable and accurate results, achieving 97% training accuracy and 94% test accuracy, with a validation loss consistently below 0.4. This narrow performance gap between training and testing demonstrated the model’s ability to generalize well without overfitting. The GRU model was ultimately selected not just for its quantitative superiority but also for its compatibility with real-time deployment constraints. Its architecture proved resilient, lightweight, and easier to optimize compared to deeper LSTM and BiLSTM counterparts, making it the preferred choice for integration into the system's live sign-to- speech communication pipeline.\\n\\nto stabilize\\n\\nB. Quantitative Evaluation\\n\\nThe performance of each model was evaluated using standard classification metrics, including accuracy, precision, recall, F1-score, and categorical crossentropy loss. Table IV Table V summarizes the comparative results:\\n\\nTable IV : Train and Test Accuracy for models\\n\\nModel\\n\\nGRU LSTM BiGRU BiLSTM\\n\\nTrain Accuracy (%) 96.67 95.01 96.04 95.12\\n\\nTest Accuracy (%) 94.56 89.44 91.21 90.09\\n\\nTable V: Precision, F1, Recall, Loss for models\\n\\nModel\\n\\nPrecision\\n\\nRecall F1\\n\\nLoss\\n\\nGRU LSTM BiGRU BiLSTM\\n\\n0.93 0.89 0.91 0.90\\n\\n0.94 0.87 0.90 0.89\\n\\nScore 0.93 0.88 0.905 0.895\\n\\n0.38 0.52 0.44 0.47\\n\\nC. Qualitative Analysis\\n\\nBeyond numerical performance, qualitative assessments were carried out during real-time usage of the GRU model\\n\\nwithin the full system pipeline. Unlike other models that generated unstable or inconsistent predictions in live testing, the GRU model consistently recognized gesture sequences with high confidence and responsiveness. Integration with motion detection further improved prediction stability by ensuring that signs were only classified when actual hand movement occurred. This eliminated the problem of continuous or erratic predictions during idle frames, which was especially common in earlier iterations without motion filtering. The GRU model also showed superior compatibility with the backend components—namely the Ollama LLM and the Streamlit-Firebase and delivering accurate outputs with minimal delay. Its smooth operation in live scenarios reinforced its selection as the final deployed model in the system.\\n\\ninterface—processing\\n\\ninputs\\n\\nVI. CONCLUSION\\n\\nThis research successfully delivers a robust and fully functional end-to-end system that bridges the communication gap between deaf-mute individuals and interviewers through real-time sign language recognition and speech synthesis. By developing a custom dataset of ten technical signs— previously absent in standard sign language—and integrating it into an intelligent gesture recognition pipeline, the system addresses a critical barrier faced by the deaf community in professional environments. After rigorous experimentation with multiple sequential deep learning models, the GRU architecture emerged as the most effective, achieving a training accuracy of 97% and a test accuracy of 94%, with minimal validation loss and exceptional stability. These results affirm its readiness for real-time deployment and scalability. The seamless integration of computer vision, deep learning, and natural language processing—powered by Streamlit, Firebase, Vosk, and the locally hosted Ollama LLM—resulted in a fluid, intuitive, and highly reliable bidirectional consistent performance across users and environments positions it as a inclusivity, breakthrough solution accessibility, and technological empowerment for the deaf and mute community.\\n\\ncommunication\\n\\nsystem.\\n\\nIts\\n\\nin\\n\\nthe pursuit of\\n\\nVII. FUTURE SCOPE\\n\\nWhile the current system shows promising results in real- time scenarios, several avenues exist for future enhancement. Expanding the vocabulary beyond 10 technical signs and enabling continuous sign sentence recognition will improve usability in more diverse professional contexts. Incorporating training with language datasets and dynamic sign\\n\\nTransformer-based architectures could further enhance model generalization. Additionally, multilingual support for text-to-speech and sign-to-text translation would allow broader adoption across linguistic regions. Integration with mobile platforms and edge devices can also make the system more accessible in remote or resource-constrained environments. Finally, user- specific adaptation—where the system fine-tunes based on individual increase personalization and recognition accuracy.\\n\\nsigning\\n\\nstyles—can\\n\\nsignificantly\\n\\nREFERENCES\\n\\n[1] R. Taneja, A. Malik, and S. Bose, “Attention-Enhanced BiGRU for Dynamic Hand Gesture Recognition Using Skeletal Data,” IEEE Transactions on Multimedia, vol. 25, pp. 4102–4115, 2023.\\n\\n[2] N. Gupta, R. Jain, and S. Shah, “Capsule Networks for Static Sign Language Recognition on RWTH-BOSTON Dataset,” International Journal of Future Generation Communications and Networking, vol. 12, no. 2, pp. 93– 101, 2024.\\n\\n[3] M. K. Sinha and V. Deshmukh, “Spatio-Temporal Separable Convolutional Networks for Real-Time Sign Language Recognition,” IEEE Access, vol. 11, pp. 12543–12556, 2023.\\n\\n[4] K. Yadav, T. Roy, and A. Mehta, “Cross-Modal Sign Language Recognition Using Dual GRU Streams and Optical Flow Fusion,” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 148–156, 2023.\\n\\n[5] P. Rao and S. Thomas, “Temporal Graph Convolutional Networks for Sign Language Video Recognition,” IEEE International Conference on Image Processing (ICIP), pp. 1809–1813, 2023.\\n\\n[6] D. Banerjee, M. Singh, and A. Varma, “3D CNN-LSTM Hybrid for Depth-Based Sign Language Recognition Using Intel RealSense,” IEEE Sensors Journal, vol. 23, no. 4, pp. 5294–5302, 2023.\\n\\n[7] Y. Patil and K. Ghosh, “Efficient Temporal Dilated GRU for Mobile Sign Language Applications,” ACM Transactions on Embedded Computing Systems, vol. 22, no. 1, pp. 1–18, 2023.\\n\\n[8] H. Qureshi, B. Alam, and M. Rizwan, “Zero-Shot Sign Language Attribute-Based Embeddings,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10132– 10143, 2023.\\n\\nRecognition\\n\\nwith\\n\\n[9] J. Sharma, P. Rana, and A. Rathi, “Time-Series 1D CNN- GRU Model for Efficient Sign Language Recognition,” Pattern Recognition Letters, vol. 163, pp. 34–41, 2022.\\n\\n[10] L. Bose, R. Verma, and H. Srivastava, “Multimodal Emotion-Aware Sign Language Recognition with Facial Landmark Fusion,” IEEE Transactions on Affective Computing, 2024, early access.\\n\\n[11] K.\\u202fPatel and M.\\u202fSharma, “Attention-Augmented LSTM for Gesture Recognition,” IEEE Access, vol.\\u202f10, 2022.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"research_paper_final.pdf\"\n",
    "loader = UnstructuredPDFLoader(file_path)\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa32d70d-7389-4123-94fa-6883f45e8ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper is split into 41 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"The paper is split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2facfd46-6bd9-4fcb-9d8d-fae8be9a5406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lightweight vector store created.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create lightweight TF-IDF embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "vectorizer = TfidfVectorizer()\n",
    "_ = vectorizer.fit(texts)\n",
    "\n",
    "# Define lightweight embedding class\n",
    "class TFIDFEmbeddings(Embeddings):\n",
    "    def embed_documents(self, docs):\n",
    "        return vectorizer.transform(docs).toarray()\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return vectorizer.transform([query]).toarray()[0]\n",
    "\n",
    "# Step 4: Create VectorStore using FAISS (or Chroma if needed)\n",
    "embedding_model = TFIDFEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"✅ Lightweight vector store created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a71448f-5d26-4f10-b377-9bf594febb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = \"llama3.2\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "116a0400-17be-4e29-aa5d-4ff1d5d26115",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2 different versions of the given user questions to retrieve relevant documents\n",
    "    from the vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. \n",
    "    provide these new alternative questions on a new line.\n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vectorstore.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3349fa92-e08b-46c9-b5ee-efdc25d7ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag prompt template\n",
    "template = \"\"\"Answer the questions based ONLY on the following context: \n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfaefb52-cfd9-42da-a2f3-9f2de8bbbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d04a46f-eeb2-4802-ad63-60ab84b2a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question):\n",
    "    ans = display(Markdown(chain.invoke(question)))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "193b6597-6fe6-4dfa-90a5-552a6266d664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unfortunately, the provided context does not mention the author of the paper. It appears to be a research paper discussing a system for real-time communication between deaf-mute individuals and interviewers, but the identity of the authors is not mentioned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Who is the author of the paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21144b37-1120-4b88-9142-e6a82a2c4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The research paper proposes a real-time sign language recognition system that translates sign gestures into both text and speech, enabling effective interaction between interviewers and deaf-mute individuals in professional interviews.\n",
       "\n",
       "The system architecture consists of several components:\n",
       "\n",
       "1. Computer Vision: The user activates their webcam stream, which is processed using OpenCV and MediaPipe for landmark detection. Specifically, the system focuses on the hands and face regions to extract key points for gesture recognition.\n",
       "2. Deep Learning Model: The extracted landmarks are passed into a trained Gated Recurrent Unit (GRU) model, optimized through experimentation for temporal classification of custom technical signs. The GRU model achieves a training accuracy of 97% and a test accuracy of 94%.\n",
       "3. Language Model: The output of the GRU model is translated into a preliminary sentence, which is then refined by a locally hosted Ollama LLM. The language model improves the sentence structure, adds missing functional words, and outputs a coherent sentence.\n",
       "4. Text-to-Speech Engine: The final message is delivered back to the interviewer interface, where it is converted into audio using the pyttsx3 text-to-speech engine.\n",
       "\n",
       "The system architecture is designed to enable smooth, real-time communication between deaf-mute individuals and interviewers through a modular, interactive web platform. The proposed framework supports both roles with role-specific processing pipelines that integrate computer vision, deep learning, natural language processing, and cloud-based communication.\n",
       "\n",
       "To enhance grammatical fluency and coherence, the system uses a conversion dictionary that maps numeric class indices to textual words. The converted text is then passed into the language processing module for correction and refinement.\n",
       "\n",
       "The system successfully delivers a robust and fully functional end-to-end system that bridges the communication gap between deaf-mute individuals and interviewers through real-time sign language recognition and speech synthesis.\n",
       "\n",
       "Future improvements are suggested, including:\n",
       "\n",
       "* Expanding the vocabulary beyond 10 technical signs\n",
       "* Enabling continuous sign sentence recognition\n",
       "* Incorporating training with language datasets and dynamic sign Transformer-based architectures\n",
       "* Multilingual support for text-to-speech and sign-to-text translation\n",
       "* Integration with mobile platforms and edge devices\n",
       "* User-specific adaptation\n",
       "\n",
       "The research paper also presents experimental results, including real-time testing on various devices and lighting conditions, which confirms the suitability of the GRU model for conversational use.\n",
       "\n",
       "Overall, the research paper proposes a novel approach to bridge the communication gap between deaf-mute individuals and interviewers through real-time sign language recognition and speech synthesis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"Give me a detailed summary of the entire research paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5787ee3f-8937-47e4-ac1d-dfaba2c95f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded.\n",
      "The paper is split into 41 chunks.\n",
      "✅ Lightweight vector store created.\n",
      "Model has been loaded\n",
      "Retriever has been loaded\n",
      "Model thinking...\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sign language recognition system with speech synthesis and Firebase integration."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pdf(question, file_path):\n",
    "    loader = UnstructuredPDFLoader(file_path)\n",
    "    data = loader.load()\n",
    "    print(\"Data has been loaded.\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    print(f\"The paper is split into {len(chunks)} chunks.\")\n",
    "    # Step 3: Create lightweight TF-IDF embeddings\n",
    "    texts = [doc.page_content for doc in chunks]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    _ = vectorizer.fit(texts)\n",
    "    \n",
    "    # Define lightweight embedding class\n",
    "    class TFIDFEmbeddings(Embeddings):\n",
    "        def embed_documents(self, docs):\n",
    "            return vectorizer.transform(docs).toarray()\n",
    "    \n",
    "        def embed_query(self, query):\n",
    "            return vectorizer.transform([query]).toarray()[0]\n",
    "    \n",
    "    # Step 4: Create VectorStore using FAISS (or Chroma if needed)\n",
    "    embedding_model = TFIDFEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    print(\"✅ Lightweight vector store created.\")\n",
    "    \n",
    "    local_model = \"llama3.2\"\n",
    "    llm = ChatOllama(model=local_model)\n",
    "    print(\"Model has been loaded\")\n",
    "    \n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2 different versions of the given user questions to retrieve relevant documents\n",
    "    from the vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. \n",
    "    provide these new alternative questions on a new line.\n",
    "    Original question: {question}\"\"\"\n",
    "    )\n",
    "    print(\"Retriever has been loaded\")\n",
    "    \n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vectorstore.as_retriever(),\n",
    "        llm,\n",
    "        prompt=QUERY_PROMPT\n",
    "    )\n",
    "    template = \"\"\"Answer the questions based ONLY on the following context: \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    print(\"Model thinking...\\n\\n\\n\")\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    ans = display(Markdown(chain.invoke(question)))\n",
    "    return ans\n",
    "\n",
    "question = \"Tell me in detail in 10 words what is in this pdf?\"\n",
    "file_path = \"research_paper_final.pdf\"\n",
    "pdf(question, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
